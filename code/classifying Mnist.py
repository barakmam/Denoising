# -*- coding: utf-8 -*-
"""OriginAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1djqnWkRIMndY3JlN6r65nWOGkwmhwSqv

# imports
"""

import numpy as np
import torch as t
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
import matplotlib.pyplot as plt
import time
import copy

"""# Choose Device"""

# choose device
device = t.device('cuda' if t.cuda.is_available() else 'cpu')
if device.type == 'cuda':
    print("Device: {}".format(t.cuda.get_device_name(0)))
print("Device type: {}".format(device))

"""# Hyperparameters"""

batch_size = 256

random_seed = 42
np.random.seed(random_seed)
t.backends.cudnn.enabled = False
t.manual_seed(random_seed)

"""# Get Data and Dataloaders"""

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ])

mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_inds, val_inds = train_test_split(np.arange(len(mnist_dataset.train_data)), test_size=0.2)
train_inds, val_inds = t.tensor(train_inds), t.tensor(val_inds)

# get dataloaders
train_sampler = t.utils.data.SubsetRandomSampler(train_inds)
val_sampler = t.utils.data.SubsetRandomSampler(val_inds)

train_loader = t.utils.data.DataLoader(mnist_dataset, batch_size=batch_size, sampler=train_sampler)
val_loader = t.utils.data.DataLoader(mnist_dataset, batch_size=batch_size, sampler=val_sampler)
test_loader = t.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)

"""# Mnist Labels Distribution"""

# classes_distribution = np.histogram(train_loader.dataset.train_labels, bins=10, range=(0, 9), normed=True)
# plt.bar(range(10), classes_distribution[0])
# plt.title('Mnist Labels Histogram')
# plt.xlabel('Digit')
# plt.ylabel('portion')

"""# Utils Functions"""


def evaluate(NN, data_loader, criterion):
    NN.eval()
    correct = 0
    total = 0
    tot_loss = 0
    with t.no_grad():
        for x, y in data_loader:
            x, y = x.to(device), y.to(device)
            output = NN(x)
            loss = criterion(output, y)
            tot_loss += loss.item()
            correct += (output.argmax(-1) == y).sum()
            total += len(y)
        accuracy = correct / total
        avg_loss = tot_loss / len(data_loader)
    return accuracy, avg_loss


def get_decoder_input(out_E, x, y):
    # need to make some samples without watermarks.
    inds_to_remove_watermark = t.randperm(len(y))[:len(y) // 11]
    out_E[inds_to_remove_watermark] = x[inds_to_remove_watermark]
    y[inds_to_remove_watermark] = 10
    return out_E, y


def unite_losses(l2loss, celoss):
    loss = l2loss * 10 + celoss / 10
    return loss


def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    # print(cm)

    fig, ax = plt.subplots(figsize=(10, 10))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')
    fig.subplots_adjust(bottom=0.2)

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    plt.setp(ax.get_yticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    # ax.tick_params(labelsize=3)

    # Loop over data dimensions and create text annotations.
    fmt = '.3f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


"""# Classifier"""


class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 1024)
        self.b1 = nn.BatchNorm1d(1024)
        self.fc2 = nn.Linear(1024, 512)
        self.b2 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 10)
        self.b3 = nn.BatchNorm1d(10)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, img):
        x = img.view(-1, 28 * 28)
        x = self.relu(self.b1(self.fc1(x)))
        x = self.relu(self.b2(self.fc2(x)))
        x = self.fc3(x)
        x = self.softmax(x)
        return x


"""# Training the classifier"""

lr = 0.01
C_epochs = 2

C = Classifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = t.optim.Adam(C.parameters(), lr=lr)

train_acc = []
val_acc = []
train_loss = []
val_loss = []
val_min_loss = np.inf

for epoch in range(C_epochs):
    start_time = time.time()
    C.train()
    correct = 0
    train_total = 0
    train_tot_loss = 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        output = C(x)
        loss = criterion(output, y)
        loss.backward()
        train_tot_loss += loss.item()
        correct += (output.argmax(-1) == y).sum()
        train_total += len(y)
        optimizer.step()

    # train evaluation:
    acc, avg_loss = evaluate(C, train_loader, criterion)
    train_acc.append(acc)
    train_loss.append(avg_loss)

    # validation evaluation:
    acc, avg_loss = evaluate(C, val_loader, criterion)
    val_acc.append(acc)
    val_loss.append(avg_loss)

    # save model checkpoint:
    if val_loss[-1] < val_min_loss:
        val_min_loss = val_loss[-1]
        t.save(C.state_dict(), 'Classifier_dict')

    end_time = time.time()
    print(
        '[Epoch {}/{}] -> Train Loss: {:.3f}, Train Acc: {:.3f}, Validation Loss: {:.3f}, Validation Acc: {:.3f}, Time: {:.3f}'.format(
            epoch + 1, C_epochs, train_loss[-1], train_acc[-1],
            val_loss[-1], val_acc[-1], end_time - start_time))

"""# Plots"""

fig, ax = plt.subplots(2, 1, figsize=(10, 8))

ax[0].plot(range(1, C_epochs + 1), train_acc, label='Train')
ax[0].plot(range(1, C_epochs + 1), val_acc, label='Validation')
ax[0].set_title('Classifier Training Accuracy', fontsize=18)
ax[0].set_xlabel('Epoch', fontsize=18)
ax[0].set_ylabel('Accuracy', fontsize=18)
ax[0].grid()
ax[0].legend(fontsize=18)

ax[1].plot(range(1, C_epochs + 1), train_loss, label='Train')
ax[1].plot(range(1, C_epochs + 1), val_loss, label='Validation')
ax[1].set_title('Classifier Training Loss', fontsize=18)
ax[1].set_xlabel('Epoch', fontsize=18)
ax[1].set_ylabel('Loss', fontsize=18)
ax[1].grid()
ax[1].legend(fontsize=18)

fig.tight_layout()
