{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNt9AhbxrN/viKokd8SKBFc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barakmam/super-resolution/blob/main/cGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS6d4lr7G0kN"
      },
      "source": [
        "# imports:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import imageio\n",
        " \n",
        "# pytorch:\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import grad as torch_grad\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# for reproducibility:\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUw2OuucHL70"
      },
      "source": [
        "# choose device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print(\"Device: {}\".format(torch.cuda.get_device_name(0)))\n",
        "print(\"Device type: {}\".format(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TPaaNiVHL4_"
      },
      "source": [
        "dataset_path = \"./data/\"\n",
        "models_path = \"./models/\"\n",
        "print_gif = True\n",
        "\n",
        "# model specific parameters:\n",
        "lr = 2e-4\n",
        "betas = (.0001, .9)\n",
        "num_epochs = 50\n",
        "\n",
        "# other parameters:\n",
        "batch_size = 256\n",
        "n_channels = 128\n",
        "Lambda = 10\n",
        "image_size = (28, 28)\n",
        "latent_dim = 128\n",
        "discriminator_iterations = 1\n",
        "\n",
        "# which models to run:\n",
        "model_inds_to_run = [0, 1]  # 0 - WGAN, 1 - DCGAN\n",
        "\n",
        "os.makedirs(models_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Q00DrUHL28"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "train_data = torchvision.datasets.FashionMNIST('./fashion_data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.FashionMNIST('./fashion_data', train=False, transform=transform)\n",
        "dataset = torch.utils.data.ConcatDataset([train_data, test_data])\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "classes = train_data.classes\n",
        "\n",
        "num_classes = int(torch.max(train_data.targets) + 1)\n",
        "assert(len(classes) == num_classes)\n",
        "\n",
        "print(\"Number of classes: {}\".format(num_classes))\n",
        "print(\"Dataset size: {}\".format(len(dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSKOevCCHL0f"
      },
      "source": [
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "rows = 5\n",
        "cols = 5\n",
        "\n",
        "sample_dataloader = torch.utils.data.DataLoader(dataset, batch_size=rows*cols, shuffle=True)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "samples, labels = next(iter(sample_dataloader))\n",
        "for i in range(samples.size(0)):\n",
        "    ax = fig.add_subplot(rows, cols, i + 1)\n",
        "    ax.imshow(samples[i].data.cpu().numpy().squeeze(), cmap='gray')\n",
        "    ax.set_title(classes[labels[i]])\n",
        "    ax.set_axis_off()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5odEQloHLyS"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\" It is mainly based on the mobile net network as the backbone network generator.\n",
        "    Args:\n",
        "        image_size (int): The size of the image.\n",
        "        channels (int): The channels of the image. \n",
        "        num_classes (int): Number of classes for dataset. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, channels, num_classes):\n",
        "        super(Generator, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.channels = channels\n",
        "\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(100 + num_classes, 128),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(1024, channels * image_size * image_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Initializing all neural network weights.\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, labels: list = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (tensor): input tensor into the calculation.\n",
        "            labels (list):  input tensor label.\n",
        "        Returns:\n",
        "            A four-dimensional vector (N*C*H*W).\n",
        "        \"\"\"\n",
        "\n",
        "        conditional_inputs = torch.cat([inputs, self.label_embedding(labels)], dim=-1)\n",
        "        out = self.main(conditional_inputs)\n",
        "        out = out.reshape(out.size(0), self.channels, self.image_size, self.image_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                m.weight.data *= 0.1\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "                m.weight.data *= 0.1\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                m.weight.data *= 0.1\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, im_size, latent_dim, n_channels, device='cpu'):\n",
        "#         super(Generator, self).__init__()\n",
        "#         self.im_size = im_size\n",
        "#         self.latent_dim = latent_dim \n",
        "#         self.n_channels = n_channels\n",
        "#         self.device = device\n",
        "\n",
        "#         assert ((self.im_size[0] + 4) % 8 == 0) and ((self.im_size[1] + 4) % 8 == 0), \"invalid input dimensions\"\n",
        "\n",
        "#         # build nn:\n",
        "#         self.pre = nn.Sequential(\n",
        "#             nn.Linear(self.latent_dim, self.n_channels * ((self.im_size[0] + 4) // 8) * ((self.im_size[1] + 4) // 8)),\n",
        "#             nn.ReLU())\n",
        "#         # input size: (n_channels, 4, 4)\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(self.n_channels, self.n_channels, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.BatchNorm2d(self.n_channels),\n",
        "#             nn.ReLU())\n",
        "#         # input size: (n_channels, 8, 8)\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(self.n_channels, self.n_channels, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.BatchNorm2d(self.n_channels),\n",
        "#             nn.ReLU())\n",
        "#         # input size: (n_channels, 16, 16)\n",
        "#         self.out = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(self.n_channels, 1, kernel_size=2, stride=2, padding=2),\n",
        "#             nn.Tanh())\n",
        "#         # output size: (1, 28, 28)\n",
        "\n",
        "#     def forward(self, input, condition):\n",
        "#         output = self.pre(input)\n",
        "#         output = output.view(-1, self.n_channels, ((self.im_size[0] + 4) // 8), ((self.im_size[1] + 4) // 8))\n",
        "#         output = self.conv1(output)\n",
        "#         output = self.conv2(output)\n",
        "#         output = self.out(output)\n",
        "#         return output\n",
        "\n",
        "#     def sample(self, num_samples):\n",
        "#         z = torch.randn((num_samples, self.latent_dim)).to(self.device)\n",
        "#         gen_images = self.forward(z)\n",
        "#         return gen_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U_vCAylHLwS"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_size, latent_dim, n_channels): \n",
        "        super(Discriminator, self).__init__()\n",
        "        self.im_size = im_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_channels = n_channels\n",
        "\n",
        "        # build nn:\n",
        "        self.conv = nn.Sequential(\n",
        "            # input size: (1, 28, 28)\n",
        "            nn.Conv2d(1, self.n_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            # input size: (128, 14, 14)\n",
        "            nn.Conv2d(self.n_channels, self.n_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            # input size: (128, 7, 7)\n",
        "            nn.Conv2d(self.n_channels, self.n_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            # input size: (128, 7, 7)\n",
        "            nn.Conv2d(self.n_channels, self.n_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            # input size: (128, 7, 7)\n",
        "            nn.AvgPool2d(kernel_size=2))\n",
        "        # input size: (128, 3, 3)\n",
        "        self.linear = nn.Linear(self.n_channels * (im_size[0] // 8) * (im_size[1] // 8), 1)\n",
        "        # output size: (1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size = input.size()[0]\n",
        "        output = self.conv(input)\n",
        "        output = output.view(batch_size, -1)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svXWuU8-HLtu"
      },
      "source": [
        "class GAN(nn.Module):\n",
        "    def __init__(self, im_size, latent_dim, n_channels, Lambda, lr, betas, clip=None, device=torch.device(\"cpu\")):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = Generator(im_size=im_size, latent_dim=latent_dim, n_channels=n_channels, isWgan=isWgan, device=device).to(device)\n",
        "        self.discriminator = Discriminator(im_size=im_size, latent_dim=latent_dim, n_channels=n_channels, isWgan=isWgan).to(device)\n",
        "        self.G_opt = optim.Adam(self.generator.parameters(), lr=(lr if isWgan else lr*50), betas=betas)\n",
        "        self.D_opt = optim.Adam(self.discriminator.parameters(), lr=lr, betas=betas)\n",
        "        self.losses = {'G': [], 'D': [], 'GP': []}\n",
        "        self.latent_dim = latent_dim\n",
        "        self.Lambda = Lambda\n",
        "        self.clip = clip\n",
        "        self.device = device\n",
        "\n",
        "    def discriminator_iteration(self, data):\n",
        "        batch_size = data.size()[0]\n",
        "        # forward fake and real data:\n",
        "        fake_data = self.generator.sample(batch_size)\n",
        "        real_data = data.to(self.device)\n",
        "        d_fake = self.discriminator(fake_data.detach())\n",
        "        d_real = self.discriminator(real_data)\n",
        "        \n",
        "        # calculate loss:\n",
        "        gradient_penalty = self.calculate_gradient_penalty(real_data, fake_data)\n",
        "        d_loss = d_fake - d_real + gradient_penalty\n",
        "        d_loss = d_loss.mean()\n",
        "        \n",
        "        self.losses['GP'].append(gradient_penalty.data)\n",
        "        self.losses['D'].append(d_loss.data)\n",
        "\n",
        "        # optimize:\n",
        "        self.D_opt.zero_grad()\n",
        "        d_loss.backward()\n",
        "        self.D_opt.step()\n",
        "\n",
        "        return d_loss.data\n",
        "\n",
        "    def generator_iteration(self, batch_size):\n",
        "        fake_data = self.generator.sample(batch_size)\n",
        "        # forward fake samples:\n",
        "        d_fake = self.discriminator(fake_data)\n",
        "        g_loss = -d_fake.mean()\n",
        "        self.losses['G'].append(g_loss.data)\n",
        "\n",
        "        # optimize:\n",
        "        self.G_opt.zero_grad()\n",
        "        g_loss.backward()\n",
        "        self.G_opt.step()\n",
        "        return g_loss.data\n",
        "\n",
        "    def calculate_gradient_penalty(self, real_data, fake_data):\n",
        "        batch_size = real_data.size()[0]\n",
        "        alpha_size = [1 for _ in real_data.size()]\n",
        "        alpha_size[0] = batch_size\n",
        "        alpha = torch.rand(alpha_size, device=self.device)\n",
        "        \n",
        "        interpolated_data = alpha * real_data + (1 - alpha) * fake_data\n",
        "        interpolated_out = self.discriminator(interpolated_data)\n",
        "        gradients = torch_grad(outputs=interpolated_out, inputs=interpolated_data,\n",
        "                               grad_outputs=torch.ones(interpolated_out.size(), device=self.device),\n",
        "                               create_graph=True, retain_graph=True)[0]\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        gradients_norm = gradients.norm(2, dim=1)\n",
        "        return ((gradients_norm - 1) ** 2).mean()*self.Lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVZIgl-5HYb3"
      },
      "source": [
        "# WGAN\n",
        "wgan = GAN(image_size, latent_dim, n_channels, Lambda, lr, betas, device=device)\n",
        "\n",
        "models = [\n",
        "    {\n",
        "        \"name\": \"WGAN\",\n",
        "        \"model\": wgan,\n",
        "        \"filename\": models_path + \"wgan.pt\",\n",
        "        \"num_epochs\": num_epochs\n",
        "    }\n",
        "]\n",
        "\n",
        "models_to_run = [models[ind] for ind in model_inds_to_run]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAuiLtkSHYZs"
      },
      "source": [
        "for model_info in models_to_run:\n",
        "    model = model_info[\"model\"]\n",
        "    num_epochs = model_info[\"num_epochs\"]\n",
        "\n",
        "    if print_gif:\n",
        "        # fix latents to see how image generation improves during training:\n",
        "        fixed_latents = torch.randn((128, model.generator.latent_dim)).to(device)\n",
        "        gif_images = []\n",
        "\n",
        "    # train:\n",
        "    print(\"----------------- Training model \\\"{}\\\" -----------------\".format(model_info[\"name\"]))\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # run discriminator and generator:\n",
        "        for it, (data, _) in enumerate(data_loader):\n",
        "            d_loss = model.discriminator_iteration(data)\n",
        "            if (it + 1) % discriminator_iterations == 0:\n",
        "                g_loss = model.generator_iteration(batch_size)\n",
        "        \n",
        "        # print progress:\n",
        "        end_time = time.time()\n",
        "        print('[{}] [Epoch {}/{}] -> G Loss: {:.3f}, D Loss: {:.3f}, Time: {:.3f}'.format(\n",
        "            model_info[\"name\"], epoch + 1, num_epochs, g_loss, d_loss, end_time - start_time))\n",
        "        \n",
        "        if print_gif:\n",
        "            img_grid = make_grid(model.generator(fixed_latents).cpu().data)\n",
        "            img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "            img_grid = (255 * (img_grid - img_grid.min()) / (img_grid.max() - img_grid.min())).astype(np.uint8)\n",
        "            gif_images.append(img_grid)\n",
        "\n",
        "    if print_gif:\n",
        "        imageio.mimsave(models_path + model_info[\"name\"] + \"_results.gif\", gif_images)\n",
        "\n",
        "    # save model checkpoint:\n",
        "    torch.save(model.state_dict(), model_info[\"filename\"])\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    print() \n",
        "\n",
        "print(\"Finished training all models!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giy_wGmrHYW2"
      },
      "source": [
        "# plot Loss of generator and discriminator:\n",
        "fig, ax_array = plt.subplots(len(models_to_run), 1, figsize=(12, 10))\n",
        "\n",
        "for ind, model_info in enumerate(models_to_run):\n",
        "    ax_array[ind].plot(discriminator_iterations*np.arange(1, len(model_info[\"model\"].losses[\"G\"]) + 1), model_info[\"model\"].losses[\"G\"], label='Generator')\n",
        "    ax_array[ind].plot(np.arange(1, len(model_info[\"model\"].losses[\"D\"]) + 1), model_info[\"model\"].losses[\"D\"], label='Discriminator')\n",
        "    ax_array[ind].plot(np.arange(1, len(model_info[\"model\"].losses[\"GP\"]) + 1), model_info[\"model\"].losses[\"GP\"], label='Gradient Penalty')\n",
        "    ax_array[ind].set_ylabel('Loss')\n",
        "    ax_array[ind].set_xlabel('Iteration')\n",
        "    ax_array[ind].set_title(\"Loss - \" + model_info[\"name\"])\n",
        "    ax_array[ind].legend()\n",
        "    ax_array[ind].grid()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhWzpetnHYUv"
      },
      "source": [
        "for model_info in models_to_run:\n",
        "    model_info[\"model\"].load_state_dict(torch.load(model_info[\"filename\"]))\n",
        "    model_info[\"model\"].to(device)\n",
        "    model_info[\"model\"].eval()\n",
        "print('GAN models loaded succesfully')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw3O9fJ4HYSv"
      },
      "source": [
        "np.random.seed(234)\n",
        "torch.manual_seed(234)\n",
        "\n",
        "n_samples = 5\n",
        "\n",
        "for model_info in models_to_run:\n",
        "    # sample GAN directly:\n",
        "    model = model_info[\"model\"]\n",
        "    model.eval()\n",
        "    gan_samples = model.generator.sample(num_samples=n_samples).view(n_samples, 28, 28).data.cpu().numpy()\n",
        "    fig, ax_array = plt.subplots(1, n_samples, figsize=(12, 2))\n",
        "    for i in range(gan_samples.shape[0]):\n",
        "        ax_array[i].imshow(gan_samples[i], cmap='gray')\n",
        "        ax_array[i].set_axis_off()\n",
        "    plt.suptitle('Sample Examples From {}'.format(model_info[\"name\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_M9U4CKHYQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S_2Pz7GHLqc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}